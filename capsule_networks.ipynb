{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule Networks\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squash Function\n",
    "\n",
    "This is the custom activation function shown in the paper. It is called as the squash function. It is calculated using the L2 norm of the vector given to this function. \n",
    "\n",
    "\\begin{equation*}\n",
    "v_j = \\frac{||s_j||^2 * s_j}{(||s_j||^2 + 1) * ||s_j|| }\n",
    "\\end{equation*}\n",
    "\n",
    "$v_j$ is the output of capsule j where $s_j$ is total input of the capsule j.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(inputs, epsilon):\n",
    "    with tf.name_scope('squash'): #Set name_scope for tensorboard\n",
    "        input_squared_norm = tf.reduce_sum(tf.square(inputs), -2, keep_dims=True, name='squared_norm') #gets the squared L2 norm\n",
    "        scalar_factor = input_squared_norm / (1 + input_squared_norm) \n",
    "        scalar_factor = scalar_factor / tf.sqrt(input_squared_norm + epsilon)\n",
    "        squashed_input = tf.multiply(scalar_factor, inputs, name='squashed_output') #Return v_j\n",
    "    return squashed_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Capsule\n",
    "\n",
    "First we create a conv2d layer with dimensions [None, 32 * 8], 9 such kernels are created each with a stride of 2 and no padding. We use ReLU activations(The activations were not mentioned in the paper, but I assume they have used them). The capsules are then reshaped to [-1, 1152, 8, 1] shape. We add the squash activation to this layer and return the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_capsule(inputs, batch_size, outputs, vector_length, strides, num_kernels, epsilon):\n",
    "    capsules = tf.contrib.layers.conv2d(inputs,  outputs * vector_length, num_kernels, strides, padding=\"VALID\", activation_fn=tf.nn.relu) #Get the conv2d layer\n",
    "    capsules = tf.reshape(capsules, [-1, 1152, vector_length, 1], name='capsules_reshape') #reshape them to required dims\n",
    "    capsules = squash(capsules, epsilon) #Add activation\n",
    "    assert(capsules.get_shape()[1:] == [1152, 8 , 1])\n",
    "    print('Primary Capsule Shape '+str(capsules.get_shape()))\n",
    "    return capsules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(name, stddev, shape): #Driver to get weights, straight forward\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=stddev), name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Caps Layer\n",
    "\n",
    "Get the inputs and reshape them to [-1, 1152, 1, 8, 1]. We now initialize b_ij to all zeros with the following shape, [inputs.shape[0], 1152, 10, 1, 1]. We also initialize the weights to [1, 1152, 10, 8, 16] dims. the inputs that we reshaped initially will now be replicated 10 times along axis = 2. So inputs are now of the form [-1, 1152, 10, 8, 1]. The weights are replicated for batch_size along axis = 0 (Had to be done because there is no dynamic initialization).\n",
    "\n",
    "We now multiply weights and the transpose of inputs, so we get u_cap with dims [-1, 1152, 10, 16, 1]. We also have u_cap_not_passed to represent weights where the grandient should not be passed, it is initialized to u_cap. \n",
    "\n",
    "Now we need to perform dynamic routing between the primary caps and digit caps layers. This is controlled by the number of routing iterations between the 2 layers. The paper suggests use of 3 iterations.\n",
    "\n",
    "c_ij is calculated as the softmax of the output vectors in b_ij(hence dim = 2).\n",
    "\n",
    "If we are not in the last round of iteration:\n",
    "* then we multiply c_ij and u_cap_not_passed, find the sume of s_j along axis = 1, we get a matrix of dims [batch_size, 1, 10, 16, 1].\n",
    "* We squash this vector, we pass a small value epsilon to prevent divide by zero Exception. v_j_replica, replicates the v_j matrix to dims [batch_size, 1152, 10, 16, 1].\n",
    "* We now find u_v as product of u_cap_not_passed and v_j_replica transpose, we get a matrix of the dim [batch_size, 1152, 10, 1, 1]. c_ij will take softmax along dim = 2 in the next iteration.\n",
    "\n",
    "If we are in the last round of iteration: \n",
    "* We do the same as above, but return the output of the squash function, without performing any changes to output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_fc(inputs, batch_size, outputs, stddev, routing_iters, epsilon):\n",
    "    inputs = tf.reshape(inputs, [-1, 1152, 1, inputs.shape[-2].value, 1])\n",
    "    inputs_shape = inputs.get_shape()\n",
    "    print(inputs_shape)\n",
    "    b_ij = tf.constant(np.zeros([batch_size, inputs.shape[1].value, outputs, 1, 1], dtype=np.float32), name='b_ij')\n",
    "#     b_ij = tf.zeros([batch_size, inputs.shape[1].value, outputs, 1, 1], dtype=np.float32), name='b_ij')\n",
    "    W = get_weights('Weight', shape=(1, 1152, 10, 8, 16), stddev=stddev)\n",
    "\n",
    "    inputs = tf.tile(inputs, [1, 1, 10, 1, 1])\n",
    "    W = tf.tile(W, [batch_size, 1, 1, 1, 1])\n",
    "    assert(inputs.get_shape()[1:] == [1152, 10, 8, 1])\n",
    "    \n",
    "    u_cap = tf.matmul(W, inputs, transpose_a=True)\n",
    "    assert(u_cap.get_shape()[1:] == [1152, 10, 16, 1])\n",
    "\n",
    "    u_cap_not_passed = tf.stop_gradient(u_cap, name='stop_gradient')\n",
    "\n",
    "    for iter_i in range(routing_iters):\n",
    "        with tf.variable_scope('iter_' + str(iter_i)):\n",
    "            c_ij = tf.nn.softmax(b_ij, dim=2)\n",
    "            \n",
    "            if iter_i == routing_iters - 1:\n",
    "                s_j = tf.multiply(c_ij, u_cap)\n",
    "                s_j = tf.reduce_sum(s_j, axis=1, keep_dims=True)\n",
    "                assert(s_j.get_shape()[1:] == [1, 10, 16, 1])\n",
    "                v_j = squash(s_j, epsilon)\n",
    "                assert(v_j.get_shape()[1:] == [1, 10, 16, 1])\n",
    "            \n",
    "            elif iter_i < routing_iters - 1:\n",
    "                s_j = tf.multiply(c_ij, u_cap_not_passed)\n",
    "                s_j = tf.reduce_sum(s_j, axis=1, keep_dims=True)\n",
    "                v_j = squash(s_j, epsilon)\n",
    "                v_j_replica = tf.tile(v_j, [1, 1152, 1, 1, 1])\n",
    "                u_v = tf.matmul(u_cap_not_passed, v_j_replica, transpose_a=True)\n",
    "                assert(u_v.get_shape() == b_ij.get_shape())\n",
    "                b_ij += u_v\n",
    "    \n",
    "    print('Digit Caps Shape '+str(v_j.get_shape()))\n",
    "    return v_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model\n",
    "\n",
    "* Setup the place holders, x_i, y.\n",
    "* Reshape x_i to [-1, 28, 28, 1]\n",
    "* Add a ReLUConv with 256 ouputs and of size 9 * 9\n",
    "* Add the PrimaryCaps Layer\n",
    "* Add DigitCaps layer, the stddev for weight initialization is taken as 0.01\n",
    "* Return the output of the DigitCaps layer, because from here, we either train or check for accuracy.\n",
    "\n",
    "Name Scopes have been added for ease in future use and for tensorboard visualizations, which really help in better understanding this complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(batch_size, epsilon):\n",
    "    with tf.name_scope('inputs'):\n",
    "        x_i = tf.placeholder(tf.float32, shape=[None, 784], name = 'x')\n",
    "        y = tf.placeholder(tf.float32, shape=[None, 10], name = 'y')\n",
    "        \n",
    "    with tf.name_scope('reshape_input'):\n",
    "        x = tf.reshape(x_i, [-1,28,28,1], name='reshape_x')\n",
    "    \n",
    "    with tf.name_scope('ReLUConv1'):\n",
    "        conv1 = tf.nn.relu(tf.contrib.layers.conv2d(x, num_outputs=256, kernel_size=9, stride=1, padding='VALID'), name = 'ReLUConv1')\n",
    "        print('Conv1 Shape: '+str(conv1.get_shape()))\n",
    "        assert(conv1.get_shape()[1:] == [20, 20, 256])\n",
    "\n",
    "    with tf.name_scope('PrimaryCaps'):\n",
    "        primary_caps = add_capsule(conv1, batch_size = batch_size, outputs=32, vector_length=8, strides=2, num_kernels=9, epsilon=epsilon)\n",
    "        assert(primary_caps.get_shape()[1:] == [1152, 8, 1])\n",
    "\n",
    "    with tf.name_scope('DigitCaps'):\n",
    "        digit_caps = add_fc(primary_caps, batch_size = batch_size, outputs=10, stddev = 0.01, routing_iters= 3, epsilon = epsilon)\n",
    "        digit_caps = tf.reshape(digit_caps, [-1, 10, 16, 1])\n",
    "        print('Reshaped Digit Caps: '+str(digit_caps.get_shape()))\n",
    "        assert(digit_caps.get_shape()[1:] == [10, 16, 1])\n",
    "        \n",
    "    return x_i, y, digit_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This is the driver for the train function. We take epsilon to be $10^{-9}$ in this case. The function is explained below:\n",
    "\n",
    "* We get the place holders and digit caps output from the compute graph that we have set up above. \n",
    "* In the output name scope, we find the L2 norm of each the vectors of length 16. This is the required output.\n",
    "* We find the softmax along the L2 norm.\n",
    "* Reshape the formax to get of the form [batch_size, 10] (This is done in order to use it in accuracy calculations, no use in loss, but since this is the output, it is under the output name scope.).\n",
    "\n",
    "To calculate loss, we perform the following operations:\n",
    "* Get the L2 norm output and reshape to [batch_size, 10]. Now we calculate the loss using the custom loss function suggested in the paper:\n",
    "\n",
    "\\begin{equation*}\n",
    "loss_c = T_c max(0, m^+ - ||v_c||)^2 + \\lambda(1-T_c) min (0, ||v_c|| - m^-)^2\n",
    "\\end{equation*}\n",
    "\n",
    "* $loss_c$ is the loss for class 'c', so the matrix representation of loss, is to just take $T_c$ as $y$ and take $||v_c||$ as the calculated L2 norm.\n",
    "* We try to reduce the mean of the loss, for each class.\n",
    "\n",
    "Accuracy Calculations are quite straight forward and need no explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_data, batch_size, iters, m_plus, m_minus, lambda_val):\n",
    "    epsilon = 1e-9\n",
    "    x, y, digit_caps = get_model(batch_size, epsilon)\n",
    "    with tf.name_scope('output'):\n",
    "        v_l2 = tf.sqrt(tf.reduce_sum(tf.square(digit_caps), axis=2, keep_dims=True), name='output')\n",
    "        print('v_l2 shape: '+str(v_l2.get_shape()))\n",
    "        softmax_v =tf.reshape(tf.nn.softmax(v_l2, dim=1), [-1, 10], name='softmax_output')\n",
    "        print('Softmax shape: '+str(softmax_v.get_shape()))\n",
    "        assert(softmax_v.get_shape() == [batch_size, 10])\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        v_l = tf.reshape(v_l2, [-1, 10])\n",
    "        lc = y * tf.square(tf.maximum(0.0, m_plus - v_l)) + lambda_val * (1 - y) * tf.square(tf.maximum(0.0, v_l - m_minus))\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(lc, axis= 1), name='loss')\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(softmax_v, 1), tf.argmax(y, 1))\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_prediction, name = 'accuracy')\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    builder = tf.saved_model.builder.SavedModelBuilder('models/')\n",
    "    writer = tf.summary.FileWriter('logs_output_1')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer.add_graph(sess.graph)\n",
    "        for i in range(iters):\n",
    "            batch = train_data.next_batch(batch_size)\n",
    "            sess.run([train_step], feed_dict={x:batch[0], y:batch[1]})\n",
    "            if i%10 == 0:\n",
    "                acc, s = sess.run([accuracy, summ],feed_dict={x:batch[0], y:batch[1]})\n",
    "                print(\"Accuracy after \"+str(i)+\" iterations is \"+str(acc))\n",
    "                writer.add_summary(s, i)\n",
    "        builder.add_meta_graph_and_variables(sess,['model_'+str(iters)+'iters'])\n",
    "        builder.save()\n",
    "        writer.close()\n",
    "        print(\"Complete!\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function\n",
    "\n",
    "This is a standard testing function, only thing to note is that since there is no dynamic creation in TensorFlow, we have to pass tests in batches of the same size. We take the average test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(test_data, iters, batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        tf.saved_model.loader.load(sess,['model_'+str(iters)+'iters'],'models/')\n",
    "        test_len = len(test_data.images) // batch_size\n",
    "        mean_acc = 0.0\n",
    "        for i in range(test_len):\n",
    "            acc = sess.run(['accuracy/accuracy:0'], feed_dict={'inputs/x:0':test_data.images[i*128:(i+1)*128], 'inputs/y:0':test_data.labels[i*128:(i+1)*128]})[0]\n",
    "            print('Accuracy in iter '+str(i)+' is '+str(acc))\n",
    "            mean_acc += acc\n",
    "        print(\"Test Accuracy is \"+str(mean_acc/test_len))    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Driver\n",
    "\n",
    "Just run this function to train and see outputs. The hyper parameters are shown in the first few lines of this function. They are chosen as per the paper specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    m_plus = 0.9\n",
    "    m_minus = 0.1\n",
    "    lambda_val = 0.5\n",
    "    \n",
    "    if not tf.gfile.Exists('dataset/'):\n",
    "        print('creating dataset dir')\n",
    "        tf.gfile.MakeDirs('dataset')\n",
    "    print('Loading dataset')\n",
    "    mnist = input_data.read_data_sets('dataset/', one_hot=True)\n",
    "    iters = 10*len(mnist.train.images) // batch_size\n",
    "    print('Loading Complete')\n",
    "    print('Will be running for '+str(iters)+' iters')\n",
    "    if tf.gfile.Exists('models/'):\n",
    "        flag = input('Do you want to reset the model and train: ')\n",
    "        if flag == 'y' or flag == 'Y':\n",
    "            tf.gfile.DeleteRecursively('models/')\n",
    "            train(mnist.train, batch_size, iters, m_plus, m_minus, lambda_val)\n",
    "    else:\n",
    "        train(mnist.train, batch_size, iters, m_plus, m_minus, lambda_val)\n",
    "    \n",
    "    test(mnist.test, iters, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Extracting dataset/train-images-idx3-ubyte.gz\n",
      "Extracting dataset/train-labels-idx1-ubyte.gz\n",
      "Extracting dataset/t10k-images-idx3-ubyte.gz\n",
      "Extracting dataset/t10k-labels-idx1-ubyte.gz\n",
      "Loading Complete\n",
      "Will be running for 4296 iters\n",
      "Do you want to reset the model and train: n\n",
      "INFO:tensorflow:Restoring parameters from b'models/variables/variables'\n",
      "Accuracy in iter 0 is 1.0\n",
      "Accuracy in iter 1 is 0.992188\n",
      "Accuracy in iter 2 is 0.976562\n",
      "Accuracy in iter 3 is 0.984375\n",
      "Accuracy in iter 4 is 0.992188\n",
      "Accuracy in iter 5 is 0.960938\n",
      "Accuracy in iter 6 is 1.0\n",
      "Accuracy in iter 7 is 0.984375\n",
      "Accuracy in iter 8 is 0.984375\n",
      "Accuracy in iter 9 is 0.976562\n",
      "Accuracy in iter 10 is 0.984375\n",
      "Accuracy in iter 11 is 0.976562\n",
      "Accuracy in iter 12 is 0.992188\n",
      "Accuracy in iter 13 is 0.984375\n",
      "Accuracy in iter 14 is 0.976562\n",
      "Accuracy in iter 15 is 0.984375\n",
      "Accuracy in iter 16 is 0.976562\n",
      "Accuracy in iter 17 is 0.992188\n",
      "Accuracy in iter 18 is 0.992188\n",
      "Accuracy in iter 19 is 0.992188\n",
      "Accuracy in iter 20 is 0.984375\n",
      "Accuracy in iter 21 is 1.0\n",
      "Accuracy in iter 22 is 0.984375\n",
      "Accuracy in iter 23 is 1.0\n",
      "Accuracy in iter 24 is 0.992188\n",
      "Accuracy in iter 25 is 1.0\n",
      "Accuracy in iter 26 is 0.992188\n",
      "Accuracy in iter 27 is 0.976562\n",
      "Accuracy in iter 28 is 1.0\n",
      "Accuracy in iter 29 is 0.96875\n",
      "Accuracy in iter 30 is 0.96875\n",
      "Accuracy in iter 31 is 0.992188\n",
      "Accuracy in iter 32 is 0.976562\n",
      "Accuracy in iter 33 is 0.984375\n",
      "Accuracy in iter 34 is 1.0\n",
      "Accuracy in iter 35 is 0.992188\n",
      "Accuracy in iter 36 is 1.0\n",
      "Accuracy in iter 37 is 0.976562\n",
      "Accuracy in iter 38 is 0.984375\n",
      "Accuracy in iter 39 is 1.0\n",
      "Accuracy in iter 40 is 1.0\n",
      "Accuracy in iter 41 is 1.0\n",
      "Accuracy in iter 42 is 1.0\n",
      "Accuracy in iter 43 is 1.0\n",
      "Accuracy in iter 44 is 0.984375\n",
      "Accuracy in iter 45 is 1.0\n",
      "Accuracy in iter 46 is 0.984375\n",
      "Accuracy in iter 47 is 1.0\n",
      "Accuracy in iter 48 is 1.0\n",
      "Accuracy in iter 49 is 1.0\n",
      "Accuracy in iter 50 is 0.992188\n",
      "Accuracy in iter 51 is 0.96875\n",
      "Accuracy in iter 52 is 0.992188\n",
      "Accuracy in iter 53 is 1.0\n",
      "Accuracy in iter 54 is 1.0\n",
      "Accuracy in iter 55 is 1.0\n",
      "Accuracy in iter 56 is 1.0\n",
      "Accuracy in iter 57 is 1.0\n",
      "Accuracy in iter 58 is 0.992188\n",
      "Accuracy in iter 59 is 1.0\n",
      "Accuracy in iter 60 is 1.0\n",
      "Accuracy in iter 61 is 1.0\n",
      "Accuracy in iter 62 is 1.0\n",
      "Accuracy in iter 63 is 0.992188\n",
      "Accuracy in iter 64 is 1.0\n",
      "Accuracy in iter 65 is 0.992188\n",
      "Accuracy in iter 66 is 0.992188\n",
      "Accuracy in iter 67 is 1.0\n",
      "Accuracy in iter 68 is 1.0\n",
      "Accuracy in iter 69 is 1.0\n",
      "Accuracy in iter 70 is 0.992188\n",
      "Accuracy in iter 71 is 1.0\n",
      "Accuracy in iter 72 is 1.0\n",
      "Accuracy in iter 73 is 1.0\n",
      "Accuracy in iter 74 is 1.0\n",
      "Accuracy in iter 75 is 0.976562\n",
      "Accuracy in iter 76 is 0.976562\n",
      "Accuracy in iter 77 is 0.992188\n",
      "Test Accuracy is 0.991085737179\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't the most elegant way of showing the output, but yes we get an average test accuracy of 99.11% . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
